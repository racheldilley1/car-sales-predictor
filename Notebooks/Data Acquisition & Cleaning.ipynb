{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from fake_useragent import UserAgent \n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "ua = UserAgent()\n",
    "user_agent_list = [\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',\n",
    "]\n",
    "headers = {'user-agent': ua.random}\n",
    "print(ua.random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Monthly Sales for 2019 & 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_sales_df(url, year_str, tbl_num):\n",
    "    '''\n",
    "    A function that gets monthly sales of every make and model sold in the us for a specified year\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : url containg monthly sales data, in string format \n",
    "    year_str : year in string format\n",
    "    tbl_num : index number corresponding to the location of the table on the webpage (1st table is at index 0)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return a df of monthly US car sales for every make and model, months as columns\n",
    "    '''\n",
    "    response = requests.get(url, headers=headers)\n",
    "    page= response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    #find car sales data and turn it into a list\n",
    "    tbl_str = soup.find_all('tbody')[tbl_num].text\n",
    "    tbl_list = tbl_str.split('\\n')\n",
    "    tbl_list = [i for i in tbl_list if i] \n",
    "    \n",
    "    #create empty time series df for specified year\n",
    "    date_time_str = year_str + '-01'\n",
    "    start_date = datetime.datetime.strptime(date_time_str, '%Y-%m')\n",
    "    index = pd.date_range(start_date, periods=12, freq='m')\n",
    "    df = pd.DataFrame(index=index)\n",
    "    \n",
    "    #fill empty df with monthly sales for all makes and models\n",
    "    col_name = ''\n",
    "    idx = 0\n",
    "    for x in range(0,len(tbl_list)//13):\n",
    "        col_name = tbl_list[x+idx]\n",
    "        list_vals = []\n",
    "        for val in range(x+1,x+13):\n",
    "            list_vals.append(tbl_list[idx+val])\n",
    "        df[col_name]=list_vals\n",
    "        idx = idx+12\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new df called monthly_sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only monthly sales data for years 2019 and 2020\n",
    "url = 'https://www.goodcarbadcar.net/2019-us-vehicle-sales-figures-by-model/' \n",
    "df_sales = get_monthly_sales_df(url, '2019', 2)\n",
    "url = 'https://www.goodcarbadcar.net/2020-us-vehicle-sales-figures-by-model/'\n",
    "df = get_monthly_sales_df(url, '2020', 1)\n",
    "monthly_sales_df = pd.concat((df_sales, df))\n",
    "#monthly_sales_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle df of monthly sales for 2019 and 2020 to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales_df.to_pickle('../data/monthly_sales_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get All Model Yearly Sales for 2005-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_sales_df(url, year):\n",
    "    '''\n",
    "    A function that gets all year end model sales for a specified year\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : url containg year end sales data for every make and model sold in the US, in string format\n",
    "    year : year in string format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return a df of yearly US car sales for every make and model \n",
    "    columns = Model, Year, and Total_Sales\n",
    "    '''\n",
    "    #read url page into list of pandas dfs \n",
    "    response = requests.get(url, headers=headers)\n",
    "    page = response.text\n",
    "    df_list = pd.read_html(page)\n",
    "    \n",
    "    #find correct df based on number of data frames on url page and \n",
    "    if len(df_list) == 1 or year == '2019':\n",
    "        df = pd.DataFrame(df_list[0])\n",
    "    elif len(df_list) == 2:\n",
    "        df = pd.DataFrame(df_list[1])\n",
    "    else:\n",
    "        df = pd.DataFrame(df_list[len(df_list)-1])\n",
    "    \n",
    "    #special case for 2020 data because sales data is in monthly sales format for each model\n",
    "    if year == '2020':  \n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "        #find car sales data and turn it into a list\n",
    "        tbl_str = soup.find_all('tbody')[1].text\n",
    "        tbl_list = tbl_str.split('\\n')\n",
    "        tbl_list = [i for i in tbl_list if i] \n",
    "        \n",
    "        columns = ('Model', 'Year', 'Total_Sales')\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        #fill empty df with monthly sales for all makes and models\n",
    "        model_name = ''\n",
    "        idx = 0\n",
    "        index = 0\n",
    "        for x in range(0,len(tbl_list)//13):\n",
    "            model_name = tbl_list[x+idx]\n",
    "            year_sum = 0\n",
    "            for val in range(x+1,x+13):\n",
    "                month_total = tbl_list[idx+val].replace(',','')\n",
    "                month_total = int(month_total)\n",
    "                year_sum = year_sum + month_total\n",
    "            #df1 = pd.DataFrame([model_name, year_sum], columns = ['Model', year])\n",
    "            df = df.append(pd.DataFrame({'Model': model_name, 'Year': year, 'Total_Sales': year_sum}, index=[index]), \n",
    "                           ignore_index=True)\n",
    "            index = index+1\n",
    "            #df.append(df1)\n",
    "            idx = idx+12\n",
    "        return df\n",
    "    \n",
    "    #special cases for finding the location of the total sales based on years\n",
    "    if year == '2012':\n",
    "        df = df.iloc[:, [2,3]]\n",
    "    elif year == '2005':\n",
    "        df = df.iloc[:, [1,3]]\n",
    "    elif year in ['2017','2018', '2019', '2020']:\n",
    "        df = df.iloc[:, [0,4]]\n",
    "    else:\n",
    "        df = df.iloc[:, [1,2]]\n",
    "    \n",
    "    #add columns to df and year column\n",
    "    df.columns = ['Model', 'Total_Sales']\n",
    "    df['Year'] = year\n",
    "    \n",
    "    #* indicate further breakdown of sum totals, overall totals be removed further down when duplicates are removed\n",
    "    #other symbols refer to subnotes in the tables and are not apart of model names\n",
    "    symbols = ['*', '²', '¹', '^', '†', '‡']\n",
    "    for s in symbols:\n",
    "        df['Model'] = df['Model'].str.replace(s,'')\n",
    "\n",
    "    #clean model and Total_Sales column\n",
    "    df.dropna(subset=['Model'], inplace=True)\n",
    "    df['Model'] = df['Model'].str.rstrip()\n",
    "    df['Model'] = df['Model'].str.lstrip()\n",
    "    df['Total_Sales'] = df['Total_Sales'].apply(pd.to_numeric, errors='coerce') #to numeric\n",
    "    \n",
    "    #remove first in set of duplicates b/c first is a sum of a car and the hybrid model\n",
    "    df.drop_duplicates(subset='Model', keep='last', inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n",
      "<ipython-input-5-00f819422cf3>:77: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df['Model'] = df['Model'].str.replace(s,'')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model Total_Sales  Year\n",
      "0            Ford F-Series    901463.0  2005\n",
      "1      Chevrolet Silverado    705980.0  2005\n",
      "2             Toyota Camry    431703.0  2005\n",
      "3    Toyota Corolla/Matrix    341290.0  2005\n",
      "4                Dodge Ram    400543.0  2005\n",
      "..                     ...         ...   ...\n",
      "298        Volvo 60-Series       15729  2020\n",
      "299        Volvo 90-Series        3195  2020\n",
      "300             Volvo XC40       23778  2020\n",
      "301             Volvo XC60       32078  2020\n",
      "302             Volvo XC90       34251  2020\n",
      "\n",
      "[4518 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#dictionary of all urls containing all year end sales data with year as the key\n",
    "url_dict = {'2006': 'https://www.goodcarbadcar.net/2006-usa-auto-sales-rankings-by-mode/',\n",
    "            '2007': 'https://www.goodcarbadcar.net/usa-2007-vehicle-sales-rankings-by-mode/',\n",
    "            '2008': 'https://www.goodcarbadcar.net/2008-america-auto-sales-rankings-by-mode/',\n",
    "            '2009': 'https://www.goodcarbadcar.net/usa-auto-sales-rankings-by-model-2009/',\n",
    "            '2010': 'https://www.goodcarbadcar.net/2010-america-auto-sales-rankings-by-mode/',\n",
    "            '2011': 'https://www.goodcarbadcar.net/top-268-best-selling-vehicles-2011-year/',\n",
    "            '2012': 'https://www.goodcarbadcar.net/2012-usa-auto-sales-rankings-by-model7/',\n",
    "            '2013': 'https://www.goodcarbadcar.net/usa-vehicle-sales-rankings-by-model-december-2013-year-end/',\n",
    "            '2014': 'https://www.goodcarbadcar.net/usa-all-cars-sales-figures-2014-december-year-end/',\n",
    "            '2015': 'https://www.goodcarbadcar.net/usa-car-sales-by-model-2015-year-end-december/',\n",
    "            '2016': 'https://www.goodcarbadcar.net/usa-2016-vehicle-sales-by-model-manufacturer-brand/',\n",
    "            '2017': 'https://www.goodcarbadcar.net/december-2017-year-end-u-s-passenger-car-sales-rankings-top-171-best-selling-cars-america-every-car-ranked/',\n",
    "            '2018': 'https://www.goodcarbadcar.net/december-2018-the-best-selling-vehicles-in-america-every-vehicle-ranked/',\n",
    "            '2019': 'https://www.goodcarbadcar.net/2019-us-vehicle-sales-figures-by-model/',\n",
    "            '2020': 'https://www.goodcarbadcar.net/2020-us-vehicle-sales-figures-by-model/'\n",
    "           }\n",
    "\n",
    "url = 'https://www.goodcarbadcar.net/2006-usa-auto-sales-rankings-by-mode/'\n",
    "yearly_sales_df= get_model_sales_df(url = url, year = '2005') #new yearl_sales_df to store all yearly sales data\n",
    "\n",
    "years = ['2005']\n",
    "#loop through dictionary with urls and stack data frames \n",
    "for key, value in url_dict.items():\n",
    "    years.append(key)\n",
    "    df = get_model_sales_df(url = value, year = key) #get df of total sales\n",
    "    yearly_sales_df = pd.concat([yearly_sales_df, df], axis=0)\n",
    "\n",
    "print(yearly_sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Total_Sales</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ford F-Series</td>\n",
       "      <td>901463.0</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chevrolet Silverado</td>\n",
       "      <td>705980.0</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Toyota Camry</td>\n",
       "      <td>431703.0</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota Corolla/Matrix</td>\n",
       "      <td>341290.0</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dodge Ram</td>\n",
       "      <td>400543.0</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4388</th>\n",
       "      <td>Volvo 60-Series</td>\n",
       "      <td>15729</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4389</th>\n",
       "      <td>Volvo 90-Series</td>\n",
       "      <td>3195</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4390</th>\n",
       "      <td>Volvo XC40</td>\n",
       "      <td>23778</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>Volvo XC60</td>\n",
       "      <td>32078</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4392</th>\n",
       "      <td>Volvo XC90</td>\n",
       "      <td>34251</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4393 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model Total_Sales  Year\n",
       "0             Ford F-Series    901463.0  2005\n",
       "1       Chevrolet Silverado    705980.0  2005\n",
       "2              Toyota Camry    431703.0  2005\n",
       "3     Toyota Corolla/Matrix    341290.0  2005\n",
       "4                 Dodge Ram    400543.0  2005\n",
       "...                     ...         ...   ...\n",
       "4388        Volvo 60-Series       15729  2020\n",
       "4389        Volvo 90-Series        3195  2020\n",
       "4390             Volvo XC40       23778  2020\n",
       "4391             Volvo XC60       32078  2020\n",
       "4392             Volvo XC90       34251  2020\n",
       "\n",
       "[4393 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove rows with no sales\n",
    "yearly_sales_df = yearly_sales_df[yearly_sales_df.Total_Sales != 0]\n",
    "yearly_sales_df = yearly_sales_df.reset_index(drop=True) #reset index after stacking dfs\n",
    "\n",
    "#Clean total sales dataframe by removing rows containing certain strings\n",
    "remove_strings = [\"Market\",'Total','Family','Brand','Passenger Cars, SUVs, Crossovers','Minivans','Pickup Trucks',\n",
    "                 'Commercial Vans', 'COMPANY', 'MOTOR', 'GROUP', 'AMERICAN', 'AUTOMOBILES', 'JAGUAR', 'DAIMLER']\n",
    "for string in remove_strings:\n",
    "    yearly_sales_df = yearly_sales_df[~yearly_sales_df.Model.str.contains(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brand_links(org_url):\n",
    "    '''\n",
    "    A function that gets all links to makes and models for every car brand\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : url containg links to all brands\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return a dictionary of links to every make and model for every brand \n",
    "    Key = brand, value = brand url\n",
    "    '''\n",
    "    response = requests.get(org_url, headers=headers)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    brand_link_dict = dict() #empty dict for all urls\n",
    "    link_list = soup.find_all('li') #get all list objects on the url page\n",
    "    #loop through all list objects, starting at index 7 where the brand links start\n",
    "    for x in link_list[7:]:\n",
    "        brand = x.text\n",
    "        link = 'https://www.carspecs.us' + x.find('a')['href']\n",
    "        brand_link_dict[brand] = link\n",
    "        time.sleep(0.5) #pause\n",
    "    return brand_link_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_links(brand, url):\n",
    "    '''\n",
    "    A function that gets all links to every make, model, and year greater than 2005 ever sold by a specified car brand\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : url containg links to every make and model ever sold by a specified car brand, string format\n",
    "    brand : brand name in string format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return a dataframe of links to every make, model, and year for brand \n",
    "    columns = Model, Year, and Model url \n",
    "    '''\n",
    "    #load url page with all car models for specified brand url\n",
    "    response = requests.get(url, headers=headers) #random user agent\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    #find first div with all model links\n",
    "    div_list = soup.find_all('div', class_='pure-u-1 pure-u-md-1-2')\n",
    "    \n",
    "    #new df to store all makes and models for all years past 2004\n",
    "    columns = ('Model', 'Year', 'Model_url')\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    #loop through all model links for specified brand \n",
    "    for x in div_list[1].find_all('li'):\n",
    "        model = brand + ' ' + x.text #brand and model name\n",
    "        link = org_url + x.find('a')['href'] #model url\n",
    "        \n",
    "        #load model url to get list of model year urls\n",
    "        response = requests.get(link)\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        year_links = soup.find_all('li') #list of all year urls for specified model\n",
    "        \n",
    "        #loop through all year links starting at index 7\n",
    "        idx = 7\n",
    "        index = 0\n",
    "        for y in year_links[7:]:\n",
    "            year = year_links[idx].text\n",
    "            if year in years: #only add year links that are 2005 to 2020\n",
    "                model_link = 'https://www.carspecs.us' + year_links[idx].find('a')['href'] #add orginal url to string\n",
    "                df = df.append(pd.DataFrame({'Model': model, 'Year': year, 'Model_url': model_link}, index=[index]), \n",
    "                                ignore_index=True)\n",
    "                index = index+1\n",
    "                idx = idx+1\n",
    "        time.sleep(0.5) #pause \n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_specs(url,head):\n",
    "    '''\n",
    "    A function that gets all specs for a specified make, model, and year of a car\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : url containg links to every year make and model ever sold by a specified car brand, string format\n",
    "    hear : header for a page request, in dictioanry format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return a list of model specs \n",
    "    '''\n",
    "    response = requests.get(url, headers=head)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "    #set all values to nan\n",
    "    doors =passengers =speed =horsepower =drive =engine =tank =volume =length =width =height = wheelbase =float('NaN')\n",
    "    mpg = ''\n",
    "    \n",
    "    #try to find price\n",
    "    try:\n",
    "        price = soup.find(text=re.compile('starting from'))\n",
    "        price = price.findNext().text\n",
    "    \n",
    "    except:\n",
    "        price = float('NaN')\n",
    "    \n",
    "    #list of all divs containg car specs\n",
    "    div_list = soup.find('div', class_='car-details').find_all('div')\n",
    "\n",
    "    #loop through all divs and search for strings to find certain car specs and assign to varibales if found\n",
    "    for div in div_list[1:]:\n",
    "        #create list, index 0 containg the spec, and index 1 containg the spec value\n",
    "        spec_list = div.text.split('\\n')\n",
    "        spec_list = [i for i in spec_list if i] #remove empty values in list\n",
    "        if spec_list:\n",
    "            if 'RPM' not in spec_list[0]:\n",
    "                if 'Passenger Doors' in spec_list[0]:\n",
    "                    doors = int(spec_list[-1])\n",
    "                if 'Passenger Capacity' in spec_list[0]:\n",
    "                    passengers = int(spec_list[-1])\n",
    "                if 'mph' in spec_list[0]:\n",
    "                    speed = spec_list[-1]\n",
    "                if 'Horsepower' in spec_list[0]:\n",
    "                    horsepower = spec_list[-1]\n",
    "                if 'Drive type' in spec_list[0]:\n",
    "                    drive = spec_list[-1]\n",
    "                if 'combined' in spec_list[0]:\n",
    "                    mpg = spec_list[-1]\n",
    "                if 'Combined' in spec_list[0]:\n",
    "                    mpg = spec_list[-1]\n",
    "                if 'Engine type' in spec_list[0]:\n",
    "                    engine = spec_list[-1]\n",
    "                    engine = engine.replace('\\t', '')\n",
    "                if 'tank capacity' in spec_list[0]:\n",
    "                    tank = spec_list[-1]\n",
    "                if 'EPA interior' in spec_list[0]:\n",
    "                    volume = spec_list[-1]\n",
    "                if 'Length' in spec_list[0]:\n",
    "                    length = spec_list[-1]\n",
    "                if 'Width' in spec_list[0]:\n",
    "                    width = spec_list[-1]\n",
    "                if 'Height' in spec_list[0]:\n",
    "                    height = spec_list[-1]\n",
    "                if 'Wheelbase' in spec_list[0]:\n",
    "                    wheelbase = spec_list[-1]\n",
    "        \n",
    "        #if mpg not found in spec lists, find combined mpg by averaged highway and city mpg by searching for strings\n",
    "        if mpg == '':\n",
    "            mpg = soup.find(text=re.compile('highway mpg'))\n",
    "            try:\n",
    "                mpg_list = mpg.split('/ ')\n",
    "                city = mpg_list[0].lstrip('\\r\\n ')\n",
    "                highway = mpg_list[1]\n",
    "                mpg = (int(city[0:2])+int(highway[0:2]))/2\n",
    "            except:\n",
    "                mpg = float('NaN') #return nan if not found\n",
    "                \n",
    "    #time.sleep(.1+.5*random.random()) #random pause \n",
    "    \n",
    "    return [price, doors, passengers, speed, horsepower, drive, mpg, engine, tank, volume, length, width, height,wheelbase]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a dictionary of links to all models of a car brand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_url = 'https://www.carspecs.us/'\n",
    "brand_links = get_brand_links(org_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a dictionary of links to all years of a specific make and model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = ('Model', 'Year', 'Model_url')\n",
    "model_links_df = pd.DataFrame(columns=columns)\n",
    "for key, value in brand_links.items():\n",
    "    df = get_model_links(key, brand_links[key])\n",
    "    model_links_df = model_links_df.append(df, ignore_index=True)\n",
    "print(model_links_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new df containging specs of every make, model, and year of a car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.carspecs.us', port=443): Max retries exceeded with url: /cars/2008/bmw/1-series (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7faed9ca7ac0>: Failed to establish a new connection: [Errno 60] Operation timed out'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             conn = connection.create_connection(\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 60] Operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7faed9ca7ac0>: Failed to establish a new connection: [Errno 60] Operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    756\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.carspecs.us', port=443): Max retries exceeded with url: /cars/2008/bmw/1-series (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7faed9ca7ac0>: Failed to establish a new connection: [Errno 60] Operation timed out'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-61a2f7418155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_links_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_specs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     model_spec_df = model_spec_df.append(pd.DataFrame({'Model': row[0],'Year':row[1], 'url': row[2], 'drive': specs[5], 'engine': specs[7],\n",
      "\u001b[0;32m<ipython-input-10-7ffba4ed57c7>\u001b[0m in \u001b[0;36mget_model_specs\u001b[0;34m(url, head)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_model_specs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#url = 'https://www.carspecs.us//cars/2020/acura/mdx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.carspecs.us', port=443): Max retries exceeded with url: /cars/2008/bmw/1-series (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7faed9ca7ac0>: Failed to establish a new connection: [Errno 60] Operation timed out'))"
     ]
    }
   ],
   "source": [
    "columns = ('Model', 'url',  'drive','engine','price','doors', 'passengers', 'speed_sec', 'horsepower_hp', 'mpg',  \n",
    "           'tank_gal', 'volume_cuft', 'length_in', 'width_in', 'height_in','wheelbase_in')\n",
    "model_spec_df = pd.DataFrame(columns=columns) \n",
    "\n",
    "#loop through all rows to get all links for every make, model, and year and send to function to get specs\n",
    "#append specs to model spec dataframe\n",
    "idx = 0\n",
    "for index, row in model_links_df.iterrows():\n",
    "    specs = get_model_specs(row[2],headers)\n",
    "    print(index)\n",
    "    model_spec_df = model_spec_df.append(pd.DataFrame({'Model': row[0],'Year':row[1], 'url': row[2], 'drive': specs[5], 'engine': specs[7],\n",
    "                                                       'price': specs[0],'doors': specs[1],'passengers': specs[2],'speed_sec': specs[3], \n",
    "                                                       'horsepower_hp': specs[4],'mpg': specs[6],\n",
    "                                                       'tank_gal': specs[8], 'volume_cuft': specs[9], 'length_in': specs[10],\n",
    "                                                      'width_in': specs[11],'height_in': specs[12],'wheelbase_in':specs[13]}, index=[idx]), \n",
    "                                                         ignore_index=True)\n",
    "    idx = idx +1 #update index of spec df\n",
    "    \n",
    "    #pause after every 10 loops\n",
    "    if idx%10 == 0:\n",
    "        time.sleep(1+1*random.random())\n",
    "        \n",
    "        #change username, chosen randomly every 50 loops\n",
    "        if idx%500 == 0:\n",
    "            user_agent = random.choice(user_agent_list)\n",
    "            headers = {'user-agent': ua.random}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove symbols and units from data in df so data can be changed to numeric datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_df2 = model_spec_df.copy()\n",
    "replace_list = ['hp', 'mpg', 'gal.', 'cu.ft.', 'in.', ',', 'sec', '$', '$']\n",
    "\n",
    "#loop through strings to remove in df\n",
    "for s in replace_list:\n",
    "    model_spec_df2 = model_spec_df2.replace(s,'', regex=True) \n",
    "model_spec_df2['price'] = model_spec_df2['price'].str.replace('$','') #endure dollar sign is removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change all columns to numeric except for drive, engine, doors, and passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['price',  'speed_sec', 'horsepower_hp', 'mpg', 'tank_gal', 'volume_cuft', \n",
    "            'width_in', 'length_in','height_in','wheelbase_in']\n",
    "for col in num_cols:\n",
    "    model_spec_df2[col] = model_spec_df2[col].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "#doors and passengers as strings so that it can be treated as categorical data\n",
    "model_spec_df2['doors'] = model_spec_df2['doors'].astype(str)\n",
    "model_spec_df2['passengers'] = model_spec_df2['passengers'].astype(str)\n",
    "model_spec_df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left merge Sales df to specs df and find info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform intial merge on Model and Year columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_df = yearly_sales_df.merge(model_spec_df2, how = 'left',on=[\"Model\",'Year'])\n",
    "model_spec_sales_df.info()\n",
    "model_spec_sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some of the cars with the top sales did not get any spec data. This needs to be further investigated to see if further merging could be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find columns total sales df and in specs df that did not merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sales_df = (yearly_sales_df.merge(model_spec_df2, on='Model', how='outer', indicator=True)\n",
    "            .query('_merge != \"both\"').drop(columns='_merge'))\n",
    "unique_sales_df['Model'].value_counts().head(20) #find top rows that did not merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify mismatches with largest value counts and replace strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate why chevy silverado is not merging\n",
    "chevy_models = model_spec_df2[model_spec_df2['Model'].str.contains('Cheverolet S')].dropna() \n",
    "#remove strings from specs df so that Chevy Silverado has total sales data\n",
    "model_spec_df2['Model'] = model_spec_df2['Model'].str.replace(' 1500','')\n",
    "model_spec_df2['Model'] = model_spec_df2['Model'].str.replace(' 2500HD','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate why ford f-series is not merging\n",
    "ford_f_models = model_spec_df2[model_spec_df2['Model'].str.contains('Ford F')].dropna()\n",
    "#replace strings from specs df so that ford f-series has total sales data\n",
    "model_spec_df2['Model'] = model_spec_df2['Model'].str.replace('150','Series') #150 model is most popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate why Mini Coopers are not merging\n",
    "mini_models = model_spec_df2[model_spec_df2['Model'].str.contains('MINI')].dropna()\n",
    "#replace strings from specs df so that ford f-series has total sales data\n",
    "model_spec_df2['Model'] = model_spec_df2['Model'].str.replace('MINI','Mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a second merge with changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_df2 = yearly_sales_df.merge(model_spec_df2, how = 'left',on=[\"Model\",'Year'])\n",
    "model_spec_sales_df2.info()\n",
    "model_spec_sales_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with null Total Sales values\n",
    "model_spec_sales_df2 = model_spec_sales_df2[model_spec_sales_df2['Total_Sales'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill in na values with existing model data from another year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_specs = model_spec_df2.copy() \n",
    "all_model_specs['passengers'] = all_model_specs['passengers'].apply (pd.to_numeric, errors='coerce')\n",
    "all_model_specs['doors'] = all_model_specs['doors'].apply (pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_grouped = all_model_specs.groupby('Model').mean().reset_index()\n",
    "models_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in model_spec_sales_df2.iterrows():\n",
    "    i = models_grouped[models_grouped['Model']==row['Model']].index.values\n",
    "    if i:\n",
    "        specs = list(models_grouped.iloc[i[0],:])\n",
    "        specs2 = specs +[float('Nan'),float('Nan'),float('Nan'),float('Nan'),float('Nan')]\n",
    "        r = list(row) \n",
    "        #print(specs2)\n",
    "        #print(r)\n",
    "        idx = 6\n",
    "        ix = 1\n",
    "        try:\n",
    "            for x in r[4:]:\n",
    "                #print(specs[idx])\n",
    "                    if math.isnan(x): \n",
    "                        #print(specs[idx])\n",
    "                        if pd.notna(specs2[ix]):\n",
    "                            model_spec_sales_df2.iloc[index,idx] = math.floor(specs2[ix])\n",
    "                            #print(model_spec_sales_df2.iloc[index,:])\n",
    "                            \n",
    "        except:\n",
    "            try:\n",
    "                for x in r[6:]:\n",
    "                    #print(specs[idx])\n",
    "                        if math.isnan(int(x)): \n",
    "                            #print(specs[idx])\n",
    "                            if pd.notna(specs2[ix]):\n",
    "                                model_spec_sales_df2.iloc[index,idx] = math.floor(specs2[ix])\n",
    "                                #print(model_spec_sales_df2.iloc[index,:])\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            idx = idx+1\n",
    "            ix = ix+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop volume_cuft from model_specs_df because not enough data points and cant be easily estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_df2['Total_Sales'] = model_spec_sales_df2['Total_Sales'].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_spec_sales_df2.dropna(inplace=True)\n",
    "model_spec_sales_df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_dfo = model_spec_sales_df2[model_spec_sales_df2['Total_Sales'] > 25]\n",
    "model_spec_sales_dfo.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers, more than 3 std dev from mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_dfo2 = model_spec_sales_dfo[(np.abs(stats.zscore(model_spec_sales_dfo[['Total_Sales','price', 'speed_sec', \n",
    "                                                                                      'horsepower_hp', 'mpg', \n",
    "                                                                                      'tank_gal', 'length_in', \n",
    "                                                                                      'width_in', 'height_in','wheelbase_in']])) < 2.5)] #filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_dfo2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove volume_cuft because not enough data points and url because not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_dfo2.drop(labels=['volume_cuft', 'url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean drive feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_dfo2['drive'] = model_spec_sales_dfo2['drive'].str.lower()\n",
    "model_spec_sales_dfo2['drive'] = model_spec_sales_dfo2['drive'].str.lstrip()\n",
    "model_spec_sales_dfo2['drive'] = model_spec_sales_dfo2['drive'].str.rstrip()\n",
    "#model_spec_sales_dfo2 = model_spec_sales_dfo2.dropna()\n",
    "model_spec_sales_dfo2['drive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_dfo2['drive'] = model_spec_sales_dfo2['drive'].str.replace('rear-wheel','rear wheel drive')\n",
    "model_spec_sales_dfo2['drive'] = model_spec_sales_dfo2['drive'].str.replace('front-wheel','front wheel drive')\n",
    "#replace = ['full-time all wheel', 'attesa e-ts full-time all wheel', 'automatic full-time all wheel']\n",
    "\n",
    "model_spec_sales_dfo2['drive'] = model_spec_sales_dfo2['drive'].str.replace('automatic full-time all wheel',\n",
    "                                                                        'all wheel drive')\n",
    "model_spec_sales_dfo2['drive'] = model_spec_sales_dfo2['drive'].str.replace('versatrak ','')\n",
    "model_spec_sales_dfo2['drive'] = model_spec_sales_dfo2['drive'].str.replace('full-time all wheel','all wheel drive')\n",
    "model_spec_sales_dfo2['drive'] = model_spec_sales_dfo2['drive'].str.replace('real time automatic full-time four-wheel',\n",
    "                                                                        'all wheel drive')\n",
    "# for s in replace:\n",
    "#     model_spec_sales_df['drive'] = model_spec_sales_df['drive'].str.replace(s,'all wheel drive')\n",
    "model_spec_sales_dfo2['drive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_spec_sales_dfo2.where(model_spec_sales_dfo2.apply(lambda x: x['drive'].map(x['drive'].value_counts()))>=10, \"other\")\n",
    "c = model_spec_sales_dfo2['drive'].value_counts()\n",
    "model_spec_sales_dfo2['drive'] = np.where(model_spec_sales_dfo2['drive'].isin(c.index[c<10]), 'other',\n",
    "                                          model_spec_sales_dfo2['drive'])\n",
    "\n",
    "model_spec_sales_dfo2['drive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_dfo2.info()\n",
    "model_spec_sales_dfo2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "investigate engine column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_dfo2['engine'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "investigate passenger column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_dfo2['passengers'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "investigate doors column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_dfo2['doors'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_spec_sales_dfo2d = model_spec_sales_dfo2.dropna()\n",
    "model_spec_sales_dfo2d = model_spec_sales_dfo2d[model_spec_sales_dfo2d.duplicated(subset=['Model','Year'], keep=False)]\n",
    "model_spec_sales_dfo2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_spec_sales_dfo3 = model_spec_sales_dfo2.dropna()\n",
    "model_spec_sales_dfo3 = model_spec_sales_dfo2.copy()\n",
    "model_spec_sales_dfo3 = model_spec_sales_dfo3[model_spec_sales_dfo3.doors != 'nan']\n",
    "model_spec_sales_dfo3 = model_spec_sales_dfo3[model_spec_sales_dfo3.passengers != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_dfo3.info()\n",
    "model_spec_sales_dfo3.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add car classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification(row):\n",
    "    l = row[12]\n",
    "    wb = row[15]\n",
    "    if math.isnan(l) or math.isnan(wb):\n",
    "        return float('NaN')\n",
    "    clas = ''\n",
    "    if l > 195 and wb > 110:\n",
    "        clas = 'large'\n",
    "    elif l >= 180 and wb>= 105:\n",
    "        clas = 'midsize'\n",
    "    else:\n",
    "        clas = 'small'\n",
    "        \n",
    "    return clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_df4 = model_spec_sales_dfo3.copy()\n",
    "model_spec_sales_df4['Class'] = model_spec_sales_df4.apply(get_classification, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_spec_sales_df4['passengers'] = model_spec_sales_df4['passengers'].apply (pd.to_numeric, errors='coerce')\n",
    "# model_spec_sales_df4['doors'] = model_spec_sales_df4['doors'].apply (pd.to_numeric, errors='coerce')\n",
    "\n",
    "model_spec_sales_df4.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle model sales and specs df to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec_sales_df4.to_pickle('../data/model_spec_sales_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
